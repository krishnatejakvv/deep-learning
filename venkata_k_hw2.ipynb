{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - K. V. V. Krishna Teja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"TensorFlow version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset\n",
    "The data from Image_Segmentation.csv is imported and sorted out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "DataFrame = pd.read_csv(\"Image_Segmentation.csv\")\n",
    "# y = target values, last column of the data frame\n",
    "y= DataFrame.iloc[:, -1]\n",
    "# X = feature values, all the columns except the last column\n",
    "x= DataFrame[['A1', 'A2','A3','A4', 'A5', 'A6','A7', 'A8','A9','A10','A11','A12', 'A13', 'A14','A15', 'A16', 'A17','A18', 'A19' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now scale the dataframe using the MinMaxScaler from sklearn.preprocessing and add bias to this normalized dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalising...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A0</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>A16</th>\n",
       "      <th>A17</th>\n",
       "      <th>A18</th>\n",
       "      <th>A19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857708</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028517</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.415698</td>\n",
       "      <td>0.382496</td>\n",
       "      <td>0.498527</td>\n",
       "      <td>0.359314</td>\n",
       "      <td>0.472015</td>\n",
       "      <td>0.627059</td>\n",
       "      <td>0.148008</td>\n",
       "      <td>0.498527</td>\n",
       "      <td>0.318996</td>\n",
       "      <td>0.168487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.442688</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.007453</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.006197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.789179</td>\n",
       "      <td>0.184706</td>\n",
       "      <td>0.538899</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.794466</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032320</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.857733</td>\n",
       "      <td>0.816045</td>\n",
       "      <td>0.926362</td>\n",
       "      <td>0.823850</td>\n",
       "      <td>0.272388</td>\n",
       "      <td>0.663529</td>\n",
       "      <td>0.292220</td>\n",
       "      <td>0.926362</td>\n",
       "      <td>0.199347</td>\n",
       "      <td>0.124946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.122530</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058935</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.201242</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.303899</td>\n",
       "      <td>0.288493</td>\n",
       "      <td>0.350515</td>\n",
       "      <td>0.268901</td>\n",
       "      <td>0.630597</td>\n",
       "      <td>0.427059</td>\n",
       "      <td>0.309298</td>\n",
       "      <td>0.350515</td>\n",
       "      <td>0.266914</td>\n",
       "      <td>0.175487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.237154</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049430</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.058385</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.345727</td>\n",
       "      <td>0.322528</td>\n",
       "      <td>0.407953</td>\n",
       "      <td>0.301637</td>\n",
       "      <td>0.563433</td>\n",
       "      <td>0.511765</td>\n",
       "      <td>0.240987</td>\n",
       "      <td>0.407953</td>\n",
       "      <td>0.302925</td>\n",
       "      <td>0.171556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A0        A1        A2   A3        A4   A5        A6        A7        A8  \\\n",
       "0   1  0.857708  0.695833  0.0  0.333333  0.0  0.028517  0.000552  0.024845   \n",
       "1   1  0.442688  0.495833  0.0  0.000000  0.0  0.009506  0.000253  0.007453   \n",
       "2   1  0.794466  0.125000  0.0  0.000000  0.0  0.032320  0.000779  0.024845   \n",
       "3   1  0.122530  0.675000  0.0  0.000000  0.0  0.058935  0.001796  0.201242   \n",
       "4   1  0.237154  0.775000  0.0  0.000000  0.0  0.049430  0.001528  0.058385   \n",
       "\n",
       "         A9       A10       A11       A12       A13       A14       A15  \\\n",
       "0  0.000393  0.415698  0.382496  0.498527  0.359314  0.472015  0.627059   \n",
       "1  0.000263  0.006197  0.000000  0.016937  0.000779  0.789179  0.184706   \n",
       "2  0.000740  0.857733  0.816045  0.926362  0.823850  0.272388  0.663529   \n",
       "3  0.004869  0.303899  0.288493  0.350515  0.268901  0.630597  0.427059   \n",
       "4  0.001389  0.345727  0.322528  0.407953  0.301637  0.563433  0.511765   \n",
       "\n",
       "        A16       A17       A18       A19  \n",
       "0  0.148008  0.498527  0.318996  0.168487  \n",
       "1  0.538899  0.016937  1.000000  0.154604  \n",
       "2  0.292220  0.926362  0.199347  0.124946  \n",
       "3  0.309298  0.350515  0.266914  0.175487  \n",
       "4  0.240987  0.407953  0.302925  0.171556  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Normalising...')\n",
    "minmax_scaler = MinMaxScaler()\n",
    "x_norm = minmax_scaler.fit_transform(x)\n",
    "x_norm = pd.DataFrame(x_norm, columns=['A1', 'A2','A3','A4', 'A5', 'A6','A7', 'A8','A9','A10','A11','A12', 'A13', 'A14','A15', 'A16', 'A17','A18', 'A19' ])\n",
    "x_norm.insert(0,'A0',1)\n",
    "x_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>A16</th>\n",
       "      <th>A17</th>\n",
       "      <th>A18</th>\n",
       "      <th>A19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>218</td>\n",
       "      <td>178</td>\n",
       "      <td>9</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.547722</td>\n",
       "      <td>1.111109</td>\n",
       "      <td>0.544331</td>\n",
       "      <td>59.629630</td>\n",
       "      <td>52.444443</td>\n",
       "      <td>75.222220</td>\n",
       "      <td>51.222220</td>\n",
       "      <td>-21.555555</td>\n",
       "      <td>46.77778</td>\n",
       "      <td>-25.222221</td>\n",
       "      <td>75.222220</td>\n",
       "      <td>0.318996</td>\n",
       "      <td>-2.040554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>130</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.250924</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.365148</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-2.666667</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>-2.333333</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.123254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.944448</td>\n",
       "      <td>0.772202</td>\n",
       "      <td>1.111112</td>\n",
       "      <td>1.025597</td>\n",
       "      <td>123.037040</td>\n",
       "      <td>111.888885</td>\n",
       "      <td>139.777790</td>\n",
       "      <td>117.444440</td>\n",
       "      <td>-33.444443</td>\n",
       "      <td>50.22222</td>\n",
       "      <td>-16.777779</td>\n",
       "      <td>139.777790</td>\n",
       "      <td>0.199347</td>\n",
       "      <td>-2.299918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>173</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.722222</td>\n",
       "      <td>1.781593</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.749488</td>\n",
       "      <td>43.592594</td>\n",
       "      <td>39.555557</td>\n",
       "      <td>52.888890</td>\n",
       "      <td>38.333336</td>\n",
       "      <td>-12.111111</td>\n",
       "      <td>27.88889</td>\n",
       "      <td>-15.777778</td>\n",
       "      <td>52.888890</td>\n",
       "      <td>0.266914</td>\n",
       "      <td>-1.998858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>197</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>1.515353</td>\n",
       "      <td>2.611111</td>\n",
       "      <td>1.925463</td>\n",
       "      <td>49.592594</td>\n",
       "      <td>44.222220</td>\n",
       "      <td>61.555557</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>-16.111110</td>\n",
       "      <td>35.88889</td>\n",
       "      <td>-19.777779</td>\n",
       "      <td>61.555557</td>\n",
       "      <td>0.302925</td>\n",
       "      <td>-2.022274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A1   A2  A3        A4   A5        A6        A7        A8        A9  \\\n",
       "0  218  178   9  0.111111  0.0  0.833333  0.547722  1.111109  0.544331   \n",
       "1  113  130   9  0.000000  0.0  0.277778  0.250924  0.333333  0.365148   \n",
       "2  202   41   9  0.000000  0.0  0.944448  0.772202  1.111112  1.025597   \n",
       "3   32  173   9  0.000000  0.0  1.722222  1.781593  9.000000  6.749488   \n",
       "4   61  197   9  0.000000  0.0  1.444444  1.515353  2.611111  1.925463   \n",
       "\n",
       "          A10         A11         A12         A13        A14       A15  \\\n",
       "0   59.629630   52.444443   75.222220   51.222220 -21.555555  46.77778   \n",
       "1    0.888889    0.000000    2.555556    0.111111  -2.666667   5.00000   \n",
       "2  123.037040  111.888885  139.777790  117.444440 -33.444443  50.22222   \n",
       "3   43.592594   39.555557   52.888890   38.333336 -12.111111  27.88889   \n",
       "4   49.592594   44.222220   61.555557   43.000000 -16.111110  35.88889   \n",
       "\n",
       "         A16         A17       A18       A19  \n",
       "0 -25.222221   75.222220  0.318996 -2.040554  \n",
       "1  -2.333333    2.555556  1.000000 -2.123254  \n",
       "2 -16.777779  139.777790  0.199347 -2.299918  \n",
       "3 -15.777778   52.888890  0.266914 -1.998858  \n",
       "4 -19.777779   61.555557  0.302925 -2.022274  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For comparison, here's the original set\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding...\n",
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "class:  6\n"
     ]
    }
   ],
   "source": [
    "print('One-hot encoding...')\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# define example\n",
    "data = array(y)\n",
    "#print(data)\n",
    "# one hot encode\n",
    "encoded = to_categorical(data)\n",
    "print(encoded)\n",
    "# invert encoding (change value of encode[] to see inveterd output)\n",
    "inverted = argmax(encoded[0])\n",
    "print('class: ', inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into train and test sets...\n"
     ]
    }
   ],
   "source": [
    "print('Splitting into train and test sets...')\n",
    "trainx = np.matrix(x_norm.iloc[0:2000])\n",
    "trainy = np.matrix(y.iloc[0:2000]).transpose()\n",
    "testx = np.matrix(x_norm.iloc[2000:2301])\n",
    "testy = np.matrix(y.iloc[2000:2301]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1 with 1 hidden layer of 5 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1: ...\n",
      "300/300 [==============================] - 0s 210us/sample - loss: 0.9847 - accuracy: 0.7767\n",
      "300/300 [==============================] - 0s 200us/sample - loss: 1.0193 - accuracy: 0.6433\n",
      "300/300 [==============================] - 0s 197us/sample - loss: 1.0379 - accuracy: 0.6667\n",
      "300/300 [==============================] - 0s 197us/sample - loss: 1.0202 - accuracy: 0.7167\n",
      "300/300 [==============================] - 0s 213us/sample - loss: 1.0099 - accuracy: 0.6533\n",
      "300/300 [==============================] - 0s 437us/sample - loss: 1.0234 - accuracy: 0.7267\n",
      "300/300 [==============================] - 0s 257us/sample - loss: 1.0385 - accuracy: 0.7200\n",
      "300/300 [==============================] - 0s 197us/sample - loss: 1.0267 - accuracy: 0.7000\n",
      "300/300 [==============================] - 0s 207us/sample - loss: 1.1287 - accuracy: 0.6033\n",
      "300/300 [==============================] - 0s 200us/sample - loss: 1.0227 - accuracy: 0.7033\n",
      "[0.78, 0.64, 0.67, 0.72, 0.65, 0.73, 0.72, 0.7, 0.6, 0.7]\n",
      "Avg Accuracy:  69.10000443458557\n"
     ]
    }
   ],
   "source": [
    "print('Experiment 1: ...')\n",
    "i = 10\n",
    "mdl1 = []\n",
    "while i>0:\n",
    "    input_size = 20\n",
    "    output_size = 8\n",
    "    hidden_layer_size = 5\n",
    "# Visit https://keras.io/initializers/#initializer for more options on initializers\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, activation = 'sigmoid',\n",
    "                    input_shape=(20,),\n",
    "                    kernel_initializer=initializers.lecun_normal(seed=None)))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(trainx, trainy, epochs=100, batch_size=100, verbose=0 )\n",
    "    mdl1_loss, mdl1_accuracy = model.evaluate(testx,testy, batch_size=None)\n",
    "    mdl1.append(mdl1_accuracy)\n",
    "    i=i-1\n",
    "print(list(np.around(mdl1,2)))\n",
    "print('Avg Accuracy: ', np.mean(mdl1)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "Accuracy is not very high; shall now adjust for learning rate with the chosen optimizer instead od default values.\n",
    "\n",
    "optimizer='adam' --> custom_optimizer = tf.keras.optimizers.(Adam/Nadam/Adamax)(learning_rate=0.1)\n",
    "For this dataset, 'Adadelta' is not very accurate... keep in mind\n",
    "\n",
    "Visit: https://keras.io/optimizers/ for more options on optimizers such as Adam, Adagrad, Adadelta, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1 (adjusted): ...\n",
      "300/300 [==============================] - 0s 197us/sample - loss: 0.1846 - accuracy: 0.9467\n",
      "300/300 [==============================] - 0s 323us/sample - loss: 0.1564 - accuracy: 0.9433\n",
      "300/300 [==============================] - 0s 250us/sample - loss: 0.2299 - accuracy: 0.9267\n",
      "300/300 [==============================] - 0s 237us/sample - loss: 0.1816 - accuracy: 0.9300\n",
      "300/300 [==============================] - 0s 887us/sample - loss: 0.1935 - accuracy: 0.9267\n",
      "300/300 [==============================] - 0s 213us/sample - loss: 0.1891 - accuracy: 0.9433\n",
      "300/300 [==============================] - 0s 240us/sample - loss: 0.1753 - accuracy: 0.9400\n",
      "300/300 [==============================] - 0s 267us/sample - loss: 0.2050 - accuracy: 0.9300\n",
      "300/300 [==============================] - 0s 213us/sample - loss: 0.2392 - accuracy: 0.9233\n",
      "300/300 [==============================] - 0s 217us/sample - loss: 0.1599 - accuracy: 0.9567\n",
      "[0.95, 0.94, 0.93, 0.93, 0.93, 0.94, 0.94, 0.93, 0.92, 0.96]\n",
      "Avg Accuracy:  93.66666674613953\n"
     ]
    }
   ],
   "source": [
    "print('Experiment 1 (adjusted): ...')\n",
    "i = 10\n",
    "mdl1 = []\n",
    "while i>0:\n",
    "    input_size = 20\n",
    "    output_size = 8\n",
    "    hidden_layer_size = 5\n",
    "# Visit https://keras.io/initializers/#initializer for more options on initializers    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, activation = 'sigmoid',\n",
    "                    input_shape=(20,),\n",
    "                    kernel_initializer=initializers.lecun_normal(seed=None)))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    custom_optimizer = tf.keras.optimizers.Nadam(learning_rate=0.1) #learning rate hyper-parameter fixed at 0.1\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=custom_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(trainx, trainy, epochs=100, batch_size=100, verbose=0 )\n",
    "    mdl1_loss, mdl1_accuracy = model.evaluate(testx,testy, batch_size=None)\n",
    "    mdl1.append(mdl1_accuracy)\n",
    "    i=i-1\n",
    "print(list(np.around(mdl1,2))) #Rounding to nearest 10^-2 decimal\n",
    "print('Avg Accuracy: ', np.mean(mdl1)*100) #Calculating mean for appended list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy has now been boosted to 93.47% as compared to the previous 68.77%.\n",
    "\n",
    "Using this format for the remaining models should work...\n",
    "\n",
    "Experiment 2 with 1 hidden layer of 25 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 2: ...\n",
      "300/300 [==============================] - 0s 253us/sample - loss: 0.1033 - accuracy: 0.9667\n",
      "300/300 [==============================] - 0s 977us/sample - loss: 0.1406 - accuracy: 0.9500\n",
      "300/300 [==============================] - 0s 237us/sample - loss: 0.1115 - accuracy: 0.9567\n",
      "300/300 [==============================] - 0s 233us/sample - loss: 0.1180 - accuracy: 0.9500\n",
      "300/300 [==============================] - 0s 227us/sample - loss: 0.0895 - accuracy: 0.9667\n",
      "300/300 [==============================] - 0s 197us/sample - loss: 0.1358 - accuracy: 0.9500\n",
      "300/300 [==============================] - 0s 253us/sample - loss: 0.1260 - accuracy: 0.9567\n",
      "300/300 [==============================] - 0s 217us/sample - loss: 0.1406 - accuracy: 0.9500\n",
      "300/300 [==============================] - 0s 830us/sample - loss: 0.1327 - accuracy: 0.9633\n",
      "300/300 [==============================] - 0s 223us/sample - loss: 0.1413 - accuracy: 0.9567\n",
      "[0.97, 0.95, 0.96, 0.95, 0.97, 0.95, 0.96, 0.95, 0.96, 0.96]\n",
      "Avg Accuracy:  95.6666648387909\n"
     ]
    }
   ],
   "source": [
    "print('Experiment 2: ...')\n",
    "i = 10\n",
    "mdl2 = []\n",
    "while i>0:\n",
    "    input_size = 20\n",
    "    output_size = 8\n",
    "    hidden_layer_size = 25\n",
    "# Visit https://keras.io/initializers/#initializer for more options on initializers    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, activation = 'sigmoid',\n",
    "                    input_shape=(20,),\n",
    "                    kernel_initializer=initializers.lecun_normal(seed=None)))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    custom_optimizer = tf.keras.optimizers.Nadam(learning_rate=0.1)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=custom_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(trainx, trainy, epochs=100, batch_size=100, verbose=0 )\n",
    "    mdl2_loss, mdl2_accuracy = model.evaluate(testx,testy, batch_size=None)\n",
    "    mdl2.append(mdl2_accuracy)\n",
    "    i=i-1\n",
    "print(list(np.around(mdl2,2)))\n",
    "print('Avg Accuracy: ', np.mean(mdl2)*100) #Calculating mean for appended list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 3 with 2 hidden layers and 5 neurons each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 3: ...\n",
      "300/300 [==============================] - 0s 227us/sample - loss: 0.1780 - accuracy: 0.9400\n",
      "300/300 [==============================] - 0s 250us/sample - loss: 0.3554 - accuracy: 0.8700\n",
      "300/300 [==============================] - 0s 210us/sample - loss: 0.2024 - accuracy: 0.9367\n",
      "300/300 [==============================] - 0s 223us/sample - loss: 0.1335 - accuracy: 0.9467\n",
      "300/300 [==============================] - 0s 247us/sample - loss: 0.1741 - accuracy: 0.9433\n",
      "300/300 [==============================] - 0s 213us/sample - loss: 0.2002 - accuracy: 0.9300\n",
      "300/300 [==============================] - 0s 207us/sample - loss: 0.1667 - accuracy: 0.9500\n",
      "300/300 [==============================] - 0s 217us/sample - loss: 0.1667 - accuracy: 0.9267\n",
      "300/300 [==============================] - 0s 233us/sample - loss: 0.1800 - accuracy: 0.9500\n",
      "300/300 [==============================] - 0s 230us/sample - loss: 0.4123 - accuracy: 0.8600\n",
      "[0.94, 0.87, 0.94, 0.95, 0.94, 0.93, 0.95, 0.93, 0.95, 0.86]\n",
      "Avg Accuracy:  92.5333321094513\n"
     ]
    }
   ],
   "source": [
    "print('Experiment 3: ...')\n",
    "i = 10\n",
    "mdl3 = []\n",
    "while i>0:\n",
    "    input_size = 20\n",
    "    output_size = 8\n",
    "    hidden_layer_size = 5\n",
    "# Visit https://keras.io/initializers/#initializer for more options on initializers    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, activation = 'sigmoid',\n",
    "                    input_shape=(20,),\n",
    "                    kernel_initializer=initializers.lecun_normal(seed=None)))\n",
    "    model.add(Dense(hidden_layer_size, activation='sigmoid'))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    custom_optimizer = tf.keras.optimizers.Nadam(learning_rate=0.1)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=custom_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(trainx, trainy, epochs=100, batch_size=100, verbose=0 )\n",
    "    mdl3_loss, mdl3_accuracy = model.evaluate(testx,testy, batch_size=None)\n",
    "    mdl3.append(mdl3_accuracy)\n",
    "    i=i-1\n",
    "print(list(np.around(mdl3,2)))\n",
    "print('Avg Accuracy: ', np.mean(mdl3)*100) #Calculating mean for appended list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 4 with 2 hidden layers and 25 neurons each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 4: ...\n",
      "300/300 [==============================] - 0s 220us/sample - loss: 0.2240 - accuracy: 0.9233\n",
      "300/300 [==============================] - 0s 303us/sample - loss: 0.0790 - accuracy: 0.9667\n",
      "300/300 [==============================] - 0s 213us/sample - loss: 0.2345 - accuracy: 0.9100\n",
      "300/300 [==============================] - 0s 220us/sample - loss: 0.0902 - accuracy: 0.9800\n",
      "300/300 [==============================] - 0s 237us/sample - loss: 0.0681 - accuracy: 0.9767\n",
      "300/300 [==============================] - 0s 283us/sample - loss: 0.0705 - accuracy: 0.9733\n",
      "300/300 [==============================] - 0s 213us/sample - loss: 0.1670 - accuracy: 0.9400\n",
      "300/300 [==============================] - 0s 223us/sample - loss: 0.1098 - accuracy: 0.9667\n",
      "300/300 [==============================] - 0s 210us/sample - loss: 0.0968 - accuracy: 0.9700\n",
      "300/300 [==============================] - 0s 263us/sample - loss: 0.0845 - accuracy: 0.9667\n",
      "[0.92, 0.97, 0.91, 0.98, 0.98, 0.97, 0.94, 0.97, 0.97, 0.97]\n",
      "Avg Accuracy:  95.73332667350769\n"
     ]
    }
   ],
   "source": [
    "print('Experiment 4: ...')\n",
    "i = 10\n",
    "mdl4 = []\n",
    "while i>0:\n",
    "    input_size = 20\n",
    "    output_size = 8\n",
    "    hidden_layer_size = 25\n",
    "# Visit https://keras.io/initializers/#initializer for more options on initializers    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, activation = 'sigmoid',\n",
    "                    input_shape=(20,),\n",
    "                    kernel_initializer=initializers.lecun_normal(seed=None)))\n",
    "    model.add(Dense(hidden_layer_size, activation='sigmoid'))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    custom_optimizer = tf.keras.optimizers.Nadam(learning_rate=0.1)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=custom_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(trainx, trainy, epochs=100, batch_size=100, verbose=0 )\n",
    "    mdl4_loss, mdl4_accuracy = model.evaluate(testx,testy, batch_size=None)\n",
    "    mdl4.append(mdl4_accuracy)\n",
    "    i=i-1\n",
    "print(list(np.around(mdl4,2)))\n",
    "print('Avg Accuracy: ', np.mean(mdl4)*100) #Calculating mean for appended list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options:  ['bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark-palette', 'seaborn-dark', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'seaborn', 'Solarize_Light2', 'tableau-colorblind10', '_classic_test']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose plot style from below selection and use it\n",
    "print('Options: ', plt.style.available)\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "hdn_lyrs = ['[5]', '[5][5]', '[25]', '[25][25]' ]\n",
    "accuracy = [np.mean(mdl1)*100, np.mean(mdl2)*100, np.mean(mdl3)*100, np.mean(mdl4)*100]\n",
    "\n",
    "pos = [i for i, _ in enumerate(hdn_lyrs)]\n",
    "\n",
    "plt.bar(pos, accuracy, color='g')\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Model-type\")\n",
    "\n",
    "# pyplot.xticks sets the current tick locations and labels of the x-axis\n",
    "plt.xticks(pos, hdn_lyrs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
